{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77bb7f49-6e22-4799-ba82-3381f2f51606",
   "metadata": {},
   "source": [
    "# NOAA Loading Station Information to Database\n",
    "Logan Gall\n",
    "\n",
    "27 May 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaa51d56-36d5-4e0e-8bef-310e3a8e884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import psycopg2 #for database connection\n",
    "#for downloading data (below)\n",
    "import csv\n",
    "import requests\n",
    "from collections import deque\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "485168db-f325-4118-85a5-f3c7469779b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection parameters\n",
    "dbname = 'postgres'\n",
    "user = 'postgres'\n",
    "password = 'Passwordd'\n",
    "host = 'localhost'\n",
    "\n",
    "# Read the API key from a file and remove any leading/trailing whitespace.\n",
    "# with open('../API_Keys/NOAA_Token.txt', 'r') as file:\n",
    "#    api_key = file.read().strip()\n",
    "\n",
    "# OR PASTE API KEY HERE:\n",
    "api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "709359e4-01ed-48b9-b879-abe45b0fdb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty table to store NOAA API Observations\n",
    "create_table_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS noaa_api (\n",
    "    uid VARCHAR(255) PRIMARY KEY,\n",
    "    date TIMESTAMP,\n",
    "    datatype VARCHAR(255),\n",
    "    station VARCHAR(255),\n",
    "    latitude NUMERIC,\n",
    "    longitude NUMERIC,\n",
    "    elevation VARCHAR(255),\n",
    "    name VARCHAR(255),\n",
    "    attributes TEXT, -- Using TEXT to accommodate commas and variable-length strings\n",
    "    value NUMERIC -- NUMERIC is suitable for any kind of number, you could use FLOAT if appropriate\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Connect to postgres DB\n",
    "conn = psycopg2.connect(dbname=dbname, user=user, password=password, host=host)\n",
    "\n",
    "# Open a cursor to perform database operations\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Execute the SQL to create the table if it doesn't exist\n",
    "cur.execute(create_table_sql)\n",
    "conn.commit()  # Commit immediately after creating the table\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7b424f-bec6-4315-9a88-72f52cf6ab09",
   "metadata": {},
   "source": [
    "## Loadining weather station data to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9304cef7-2524-47b7-88c0-650aacc2312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty table to store NOAA Station list\n",
    "create_table_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS noaa_station_list (\n",
    "    elevation VARCHAR(255),\n",
    "    mindate DATE,\n",
    "    maxdate DATE,\n",
    "    latitude NUMERIC,\n",
    "    name VARCHAR(255),\n",
    "    datacoverage NUMERIC,\n",
    "    id VARCHAR(255) PRIMARY KEY,\n",
    "    elevationUnit VARCHAR(50),\n",
    "    longitude NUMERIC\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Connect to postgres DB\n",
    "conn = psycopg2.connect(dbname=dbname, user=user, password=password, host=host)\n",
    "\n",
    "# Open a cursor to perform database operations\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Execute the SQL to create the table if it doesn't exist\n",
    "cur.execute(create_table_sql)\n",
    "conn.commit()  # Commit immediately after creating the table\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedf522b-9522-4ae7-bd4d-e82fb24269a1",
   "metadata": {},
   "source": [
    "### Download weather station data\n",
    "\n",
    "This code is pulled from the ETL_Download_Stack.ipynb file to download data for our weather stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0191be1e-23a4-4345-b2ac-4fe278d99600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for the API calls.\n",
    "url = 'https://www.ncdc.noaa.gov/cdo-web/api/v2/'\n",
    "\n",
    "# Headers required for the API call, including the authorization token.\n",
    "headers = {\n",
    "    'token': api_key  # API key is passed as a token in the header.\n",
    "}\n",
    "\n",
    "def api_call(url, endpoint, headers, parameters):\n",
    "    \"\"\"\n",
    "    Make an API call to the specified URL and endpoint with given headers and parameters.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The base URL for the API.\n",
    "        endpoint (str): The specific endpoint to access data from the API.\n",
    "        headers (dict): Headers to include in the request (e.g., authorization tokens).\n",
    "        parameters (dict): Query parameters to customize the request.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The JSON response from the API if the call is successful, None otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(parameters['offset'])  # Debugging: print the current offset before making the call.\n",
    "        response = requests.get(url + endpoint, headers=headers, params=parameters)  # Perform the GET request.\n",
    "        response.raise_for_status()  # Check for HTTP errors and raise exceptions for them.\n",
    "        print(\"API Called\")  # Debugging: confirm the API was called.\n",
    "        return response.json()  # Return the parsed JSON response.\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"API call failed: {e}\")  # Handle exceptions (e.g., network issues, 4xx and 5xx errors).\n",
    "        return None\n",
    "\n",
    "def append_to_csv(file_path, data, fieldnames):\n",
    "    \"\"\"\n",
    "    Append data to a CSV file. If the file doesn't exist, create it and write the headers.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "        data (list): A list of dictionaries representing the rows to append.\n",
    "        fieldnames (list): Headers or fieldnames for the CSV.\n",
    "    \"\"\"\n",
    "    with open(file_path, mode='a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        if file.tell() == 0:  # Check if the file is empty to decide if headers need to be written.\n",
    "            writer.writeheader()\n",
    "        writer.writerows(data)  # Append the data rows to the CSV file.\n",
    "\n",
    "def append_metadata_to_csv(metadata_file_path, metadata):\n",
    "    \"\"\"\n",
    "    Append metadata to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        metadata_file_path (str): Path to the CSV file where metadata is stored.\n",
    "        metadata (dict): Metadata to be appended.\n",
    "    \"\"\"\n",
    "    with open(metadata_file_path, mode='a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=metadata['resultset'].keys())\n",
    "        if file.tell() == 0:  # Check if the file is empty to decide if headers need to be written.\n",
    "            writer.writeheader()\n",
    "        writer.writerow(metadata['resultset'])  # Append metadata to the metadata CSV.\n",
    "        \n",
    "def execute_api_calls_with_retries(base_url, endpoint, headers, initial_parameters, file_path, metadata_file_path, error_log_path, max_retries=3):\n",
    "    \"\"\"\n",
    "    Execute API calls with a mechanism for retries on failure, appending successful results to a CSV file\n",
    "    and metadata to another CSV file.\n",
    "    \n",
    "    Args:\n",
    "        base_url (str): The base URL for the API calls.\n",
    "        endpoint (str): The specific endpoint to access data from the API.\n",
    "        headers (dict): Headers to include in the request.\n",
    "        initial_parameters (dict): Initial query parameters for the API call.\n",
    "        file_path (str): Path to the CSV file where results are stored.\n",
    "        metadata_file_path (str): Path to the CSV file where metadata is stored.\n",
    "        error_log_path (str): Path to the CSV file where failed attempts are logged.\n",
    "        max_retries (int): Maximum number of retry attempts for a failed API call.\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"CSV file {file_path} already exists. Operation cancelled to prevent overwriting.\")\n",
    "        return  # Prevents overwriting existing data by aborting if the file already exists.\n",
    "    \n",
    "    stack = deque([initial_parameters])  # Use a stack to manage API call parameters, starting with the initial parameters.\n",
    "    failed_attempts = []  # Track parameters that fail to succeed after the maximum number of retries.\n",
    "    \n",
    "    while stack:\n",
    "        parameters = stack.pop()  # Pop the last set of parameters to make an API call.\n",
    "        retries = parameters.pop('retries', 0)  # Extract or initialize the retry counter for these parameters.\n",
    "        data = api_call(base_url, endpoint, headers, parameters)  # Attempt the API call.\n",
    "        \n",
    "        if data and 'results' in data:  # Check if the call was successful and data was returned.\n",
    "            append_to_csv(file_path, data['results'], data['results'][0].keys())  # Append successful results to the CSV.\n",
    "            append_metadata_to_csv(metadata_file_path, data['metadata'])  # Append metadata to the metadata CSV\n",
    "            \n",
    "            # Check if there are more results to fetch and prepare the next set of parameters.\n",
    "            if parameters.get('offset', 1) + parameters.get('limit', 10) - 1 < data['metadata']['resultset']['count']:\n",
    "                next_params = parameters.copy()\n",
    "                next_params['offset'] += parameters.get('limit', 10)\n",
    "                stack.append(next_params)  # Add the next parameters to the stack for subsequent API calls.\n",
    "        else:\n",
    "            # Retry logic for failed attempts.\n",
    "            if retries < max_retries:\n",
    "                parameters['retries'] = retries + 1  # Increment the retry counter.\n",
    "                stack.append(parameters)  # Re-add the parameters to the stack for retry.\n",
    "            else:\n",
    "                print(f\"Max retries reached for parameters: {parameters}\")\n",
    "                failed_attempts.append(parameters)  # Log parameters that exceeded retry attempts.\n",
    "    \n",
    "    # Optionally log failed attempts to a specified CSV for troubleshooting.\n",
    "    if failed_attempts:\n",
    "        append_to_csv(error_log_path, failed_attempts, ['locationid', 'limit', 'offset', 'retries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70736ab9-d8de-4e24-a689-6cdd3a0d1879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to call the function\n",
    "endpoint = \"/stations/\"\n",
    "parameters = {\n",
    "    \"locationid\": \"FIPS:27\", #CHANGE OR REMOVE FIPS CODE TO DOWNLOAD STATIONS NEAR LOCATION OF YOUR CHOICE\n",
    "    \"limit\": 1000,\n",
    "    \"offset\": 1\n",
    "}\n",
    "file_path = \"stations.csv\"\n",
    "error_log_path = \"error_log.csv\"\n",
    "metadata_path = \"stations_meta.csv\"\n",
    "\n",
    "execute_api_calls_with_retries(url, endpoint, headers, parameters, file_path, metadata_path, error_log_path, max_retries=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884c2c61-3263-4e98-957f-0066cb7780e9",
   "metadata": {},
   "source": [
    "### Push weather station data to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72785a44-0ba5-4f09-8c6a-e0cfa987928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load station data to database\n",
    "csv_file_path = './stations.csv'  # Make sure this path is correct\n",
    "\n",
    "# Connect to postgres DB\n",
    "conn = psycopg2.connect(dbname=dbname, user=user, password=password, host=host)\n",
    "\n",
    "# Open a cursor to perform database operations\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Reading and inserting data from CSV\n",
    "with open(csv_file_path, newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO noaa_station_list(elevation, mindate, maxdate, latitude, name, datacoverage, id, elevationUnit, longitude)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            ON CONFLICT (id) DO NOTHING;\n",
    "        \"\"\", (row['elevation'], row['mindate'], row['maxdate'], row['latitude'], row['name'], row['datacoverage'], row['id'], row['elevationUnit'], row['longitude']))\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
